:_mod-docs-content-type: CONCEPT

[id="con-architecture-overview.adoc_{context}"]
= Architecture overview

The Orchestrator architecture is composed of several integral components, each contributing to the running and management of workflows.

{product} ({product-very-short}):: Serves as the primary interface. {product-custom-resource-type} fulfills the following roles:

* Orchestrator Plugins: Both frontend and backend plugins provide the interface and necessary data retrieval for users to run and monitor workflows within {product-very-short}.
* Notifications Plugin: Informs users about workflow events.

OpenShift Serverless Logic Operator:: Manages the Sonataflow custom resource (CR), where each CR represents a deployed workflow.

Sonataflow Runtime/Workflow Application:: Functions as a deployed workflow. Operates as an HTTP server, handling requests for running workflow instances. It is managed as a Kubernetes (K8s) deployment by the Openshift Serverless Logic Operator.

Data Index Service:: Serves as a repository for workflow definitions, instances, and associated jobs. It exposes a GraphQL API used by the Orchestrator backend plugin to retrieve workflow definitions and instances.

Job Service:: Orchestrates scheduled tasks for workflows.

OpenShift Serverless:: Provides serverless capabilities essential for workflow communication. It employs Knative eventing to interface with the Data Index service and uses Knative functions to introduce more complex logic to workflows.

PostgreSQL Server:: Provides a database solution essential for data persistence within the Orchestrator ecosystem. The system uses PostgreSQL Server for storing both Sonataflow information and {product-custom-resource-type} data.

KeyCloak:: Provides authentication and security services within applications. KeyCloak must be provisioned externally to manage authentication, as the Orchestrator Operator does not install it.

OpenShift AMQ Streams (Strimzi/Kafka):: Provides enhanced reliability of the eventing system. While the eventing can work without Kafka by using direct HTTP calls, this approach is not reliable. The AMQ Streams Operator is an optional requirement and is not natively present or integrated into the deployment current iteration.